{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "292a94d7-3dd4-4d37-961f-2312eaa0004e",
   "metadata": {},
   "source": [
    "## INFO 371 Problem Set 6\n",
    "\n",
    "*Name*: Xiaobing Xu\n",
    "    \n",
    "*Section*: INFO 371\n",
    "\n",
    "*Date*: May 22nd, 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cc5f067-fdda-48a4-a9be-fd86af83fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77901a84-69b7-45a0-ada5-8a24b25395fd",
   "metadata": {},
   "source": [
    "### 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "895efd3e-3d35-4e69-9764-2256d07c097c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>files</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>3-1msg1.txt</td>\n",
       "      <td>Subject: re : 2 . 882 s - &gt; np np  &gt; date : su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>3-1msg2.txt</td>\n",
       "      <td>Subject: s - &gt; np + np  the discussion of s - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>3-1msg3.txt</td>\n",
       "      <td>Subject: 2 . 882 s - &gt; np np  . . . for me it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>3-375msg1.txt</td>\n",
       "      <td>Subject: gent conference  \" for the listserv \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>3-378msg1.txt</td>\n",
       "      <td>Subject: query : causatives in korean  could a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    spam          files                                            message\n",
       "0  False    3-1msg1.txt  Subject: re : 2 . 882 s - > np np  > date : su...\n",
       "1  False    3-1msg2.txt  Subject: s - > np + np  the discussion of s - ...\n",
       "2  False    3-1msg3.txt  Subject: 2 . 882 s - > np np  . . . for me it ...\n",
       "3  False  3-375msg1.txt  Subject: gent conference  \" for the listserv \"...\n",
       "4  False  3-378msg1.txt  Subject: query : causatives in korean  could a..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails = pd.read_csv(\"../Data/all-data/lingspam-emails.csv.bz2\",sep='\\t')\n",
    "emails.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829637f8-e867-474c-b90e-a2b788be020c",
   "metadata": {},
   "source": [
    "### 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "057a7163-d1c8-46c8-91ca-27789b6d883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "# define vectorizer\n",
    "X = vectorizer.fit_transform(emails.message)\n",
    "Y = emails.spam.values\n",
    "# vectorize your data. Note: this creates a sparse matrix, # use .toarray() if you run into trouble\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "# in case you want to see what are the actual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89ed9749-f6de-4c74-a0ef-6a301509d81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DTM = pd.DataFrame(X.toarray(), columns=vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275755bf-4d44-485e-b34f-89d057502630",
   "metadata": {},
   "source": [
    "### 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "573bc26e-7c69-4786-865f-cdb7bf09783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xt, Xv, Yt, Yv = train_test_split(X, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71096614-f763-4f9f-a100-b42936ea638e",
   "metadata": {},
   "source": [
    "### 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cff12c0-004c-4d63-aa0a-599734fe575f",
   "metadata": {},
   "source": [
    "- `P_S1`: Pr(category = S)\n",
    "- `P_S0`: Pr(category = NS)\n",
    "- `P_W1`: Pr(word = 1)\n",
    "- `P_W0`: Pr(word = 0)\n",
    "- `P_W1_S1`: Pr(word = 1 | category = S)\n",
    "- `P_W0_S1`: Pr(word = 0 | category = S)\n",
    "- `P_W1_S0`: Pr(word = 1 | category = NS)\n",
    "- `P_W0_S0`: Pr(word = 0 | category = NS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6868a50e-f370-4561-a79e-cc48795ee654",
   "metadata": {},
   "source": [
    "### 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb537ed3-dc7a-4d76-9efd-bf6b19f7394e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Subject: summary : parsing of ambiguous sequences in v2 languages  below is a list of references that i received in response to my query about the parsing of ambiguous svo / ovs sequences in v2 languages . many thanks to the following linguists for their quick and informative replies : gisbert fanselow , edith kaan , inge lasser , ming wei lee , michael meng , weijia ni , herbert schriefers and craig thiersch . @ article { bader94 , author = \" markus bader \" , year = 1994 , title = \" syntactic function ambiguities \" , journal = \" folia linguistica \" , volume = \" 28 / 1 - - 2 \" , pages = \" 5 - - 66 \" , } @ unpublished { bayer / marslen-wilson 92 , author = \" josef bayer and william marslen - wilson \" , year = 1992 , title = \" configurationality in the light of language comprehension : the order of arguments in { g } erman \" , note = \" university of leipzig and birbeck university college london \" , } @ incollection { crocker94 , author = \" matthew w . crocker \" , year = 1994 , title = \" on the nature of the principle-based sentence processor \" , editor = \" c . clifton and lyn frazier and k . rayner \" , booktitle = \" perspectives on sentence processing \" , address = \" new york \" , publisher = \" lawrence erlbaum \" , note = \" only brief discussion of svo vs . ovs \" , } @ unpublished { gorrell95 , author = \" paul gorrell \" , year = 1995 , title = \" parsing theory and word-order variation in { g } erman \" , note = \" ms . , universit { \\\\ \" a } t potsdam \" , } @ book { farke94 , author = \" h . farke \" , year = 1994 , title = \" grammatik und { s } prachverarbeitung . { z } ur { v } erarbeitung syntaktischer { a } mbiguit { \\\\ \" a } ten \" , address = \" opladen \" , publisher = \" westdeutscher verlag \" , } @ incollection { farke / felix94 , author = \" h . farke and sascha w . felix \" , year = 1994 , title = \" subjekt - { o } bjektasymmetrien in der { s } prachverarbeitung \" , editor = \" sascha w . felix and ch . habel and g . rickheit \" , booktitle = \" kognitive { l } inguistik . { r } epr { \\\\ \" a } sentationen und { p } rozesse \" , address = \" opladen \" , publisher = \" westdeutscher verlag \" , } @ article { frazier87 , author = \" lyn frazier \" , year = 1987 , title = \" processing syntactic structures : evidence from { d } utch \" , journal = nllt , volume = 5 , pages = \" 519 - - 559 \" , } @ article { frazier93 , author = \" lyn frazier \" , year = 1993 , title = \" processing { d } utch sentence structures \" , journal = \" journal of psycholinguistic research \" , volume = 22 , pages = \" 83 - - 108 \" , } @ article { frazier / flores89 , author = \" lyn frazier and g . flores d \\' arcais \" , year = 1989 , title = \" filler - driven parsing : a study of gap-filling in { d } utch \" , journal = \" journal of memory and language \" , volume = 28 , pages = \" 331 - - 344 \" , } @ mathesis { haverkort86 , author = \" marco haverkort \" , year = 1986 , title = \" parasitic gaps : multiple variable binding , connectedness , { atb } or chain composition \" , school = \" university of nijmegen \" , } @ phdthesis { hemforth92 , author = \" b . hemforth \" , year = 1992 , title = \" kognitives { p } arsing \" , school = \" ruhr - universit { \\\\ \" a } t , bochum \" , } @ incollection { hemforth-et - al93 , author = \" b . hemforth and l . konieczny and g . strube \" , year = 1993 , title = \" incremental syntax processing and parsing strategies \" , booktitle = \" proceedings of the { xv } th annual conference of the { c } ognitive { s } cience { s } ociety \" , address = \" hilldale \" , publisher = \" lawrence erlbaum \" , } @ book { jansen81 , author = \" f . jansen \" , year = 1981 , title = \" syntaktische konstrukties in gesproken taal \" , address = \" amsterdam \" , publisher = \" huis aan de drie grachten \" , } @ incollection { jordens91 , author = \" p . jordens \" , year = 1991 , title = \" linguistic knowledge in second language acquisition \" , editor = \" l . eubank \" , booktitle = \" point counterpoint : { u } niversal { g } rammar in the second language \" , address = \" amsterdam \" , publisher = benjamins , } @ unpublished { kaan93 , author = \" edith kaan \" , year = 1993 , title = \" processing { d } utch main clauses : a self-paced reading study \" , note = \" ms . , university of { g } roningen \" , } @ unpublished { lamers-et - al95 , author = \" m . j . a . lamers and l . a . stowe and th . c . gunter \" , year = 1995 , title = \" parsing { d } utch sentences : { svo } versus { ovs } structure \" , note = \" poster presented at the 8th { cuny } { c } onference on { h } uman { s } entence { p } rocessing , { t } ucson \" , } @ unpublished { mecklinger-et - al-in - press , author = \" a . mecklinger and h . schriefers and k . steinhauer and a . friederici \" , year = \" in press \" , title = \" the processing of relative clauses varying in syntactic complexity and semantic plausibility : an analysis with event related potentials \" , journal = \" memory and cognition \" , volume = \" \" , pages = \" \" , note = \" ms . , university of berlin \" , } @ book { nieuwborg68 , author = \" e . nieuwborg \" , year = 1968 , title = \" de distributie van het onderwerp en het lijdend voorwerp \" , address = \" antwerp \" , publisher = \" plantyn \" , } @ article { schriefers-et - al-in - press , author = \" h . schriefers and a . d . friederici and k . k { \\\\ \" u } hn \" , year = \" in press \" , title = \" the processing of locally ambiguous clauses in { g } erman \" , journal = \" journal of memory and language \" , volume = \" \" , pages = \" \" , } @ article { vincenzi91 , author = \" m . { de vincenzi } \" , year = 1991 , title = \" filler - gap dependencies in a null subject language : referential and nonreferential { wh } s \" , journal = \" journal of psycholinguistic research \" , volume = 20 , pages = \" 197 - - 213 \" , }',\n",
       "       'Subject: re : sapir - whorf and what to tell students these days  here \\'s what i tell my undergrad and grad students about that same chapter in language files . i . e . , it \\'s difficult to even judge what \\'s going on with whorf unless you are simultaneously conversant with linguistics , american indian languages , and at least the insights of modern physics . first , the conclusion that is appropriate is that , as i showed in \" the demise of the whorf hypothesis \" ( berkeley linguistics society , 1978 ? ) , what whorf said has little or generally no relation whatever to the entire body of discussion that comes under the name \" ( sapir - ) whorf hypothesis \" . he showed decades before the critics came up with their own hypotheses , which they failed to name after themselves , that he would never have agreed with their characterization of his thoughts . simple test : read the lf chapter and then ask , \" who created the whorf hypothesis ? \" and a quick way to answer this is : what did whorf himself call it in his two or three references ? he called it the \" principle of linguistic relativity \" or the \" linguistic relativity principle \" . my own reading of whorf never finds the word \" hypothesis \" at all . so - - right off the bat , and this is a good way to teach scientific nomenclature , who turned whorf \\'s \\' principle \\' into a ( n ) \\' hypothesis \\' , and why ? it was n\\'t whorf , because his designation was clear . so what is the difference between the two ? well , a principle is like an axiom in geometry : a starting point which is theoretically unverifiable - - it \\'s just a starting point . you want something else , you begin from a different starting point , and then you develop your hypotheses from there . next : what does whorf \\'s \" linguistic relativity principle \" have to do , if anything , with einstein \\'s \" relativity principle \" ( which i covered in my also bls , 1980 ? , paper , \" is whorf \\'s relativity einstein \\'s relativity ? \" ) . ah , now we \\' ve gotten to the crux of it - - much against pinker \\'s stand ( which was copied and intensified in a lingua franca short bio of suzette elgin hayden recently , where a digression found its author saying that whorf cobbled together his theory from a few ill-translated snatches of apache - - echoing a pinker statement and relying on pinker \\'s quoting whorf correctly , which he did n\\'t , about a canoe on a beach pointwise : which pinker identified as an apache sentence , but which whorf knew quite well was nootka in the pacific northwest rather than apache in the beachless desert ) , whorf was upping the ante on einstein , who argued that euclidian geometry , far from being universal , was applicable only to flat surfaces ; that for round surfaces , which most of reality is made up of , you need non - euclidian geometry . i . e . , when the phenomena change significantly , you have to change the tool you \\' re using . well , that \\'s what whorf said too ( see heisenberg \\'s lament below ) , except he moved its domain from mathematics to natural human language ; hence : the truly aptly named \" principle of linguistic relativity \" as whorf himself named it . admittedly , this does n\\'t make any sense until you see it in action , in \" an american indian model of the universe , \" where he posits a worldview without our tense / time ( past / present / future diorama or river of time ) , using manifested / manifesting ( plus other synonyms ) instead . that is : sae grammars / cultures give that aforementioned notion of \" time \" , which is supported and maintained by their tense systems ( though , admittedly , english is weird and gets it through the culture side of the language / culture system ) ; the hopi language / culture system has no such image of time , working from a different worldview principle that sees only cyclical , not linear , time - - round , not flat . most of the world \\'s grammars that have broken out of the latinate mold show that the particular time / tense system of sae is pretty much peculiar to western european languages - - hence a linguistic / cultural ontology of \" time \" as we know and practice it , and not the supposed universal we have so fondly believed it to be . side note : for one who reads whorf closely , he makes five or ten times more universalist statements than relativity statements in his writings , yet he is seen ( and reviled in a chomskyan universalist attitude ) as the relativist par excellent . so right from the get-go we see that 1 ) whorf did n\\'t write and would n\\'t agree with the hypothesis that someone ( s ) named after him ; 2 ) rather than being some deranged crackpot , he was merely literate : whorf was one of the few interdisciplinary thinkers between physics and linguistics in this century ; 3 ) whorf \\'s relativity principle had something important to do with einstein \\'s ; and 4 ) whorf was a universalist as well as a relativist - - he just had them in balance , a notable enough rarity in current academe , you must admit . if i may be so bold , alluding to your posting that \" no one has disproved whorf \\'s mild version of linguistic relativity ( let \\'s leave ling . determinism aside , or the stronger version . ) \" , even the mild version was n\\'t his ! trace back like i did and you will find that whorf espoused neither strong nor weak versions of determinism , and relativity has nothing to do with determinism when you see it from the physics viewpoint above , as he did . if you read carefully , writers about the whorf hypothesis admit that even whorf did n\\'t hold a strong version of determinism ( so if he did n\\'t , who did ? and if nobody did , why bring it up ? ) , and that all the critics hold the weak version that they dreamed up ( even though whorf would n\\'t hold it because it \\'s at least weakly deterministic and therefore newtonian ) . so what \\'s going on ? the problem is that whorf had already , from his acquaintance with physics , moved from newtonian monocausal determinism as an ideal into systems thinking - - where sometimes the opposite of one profound truth is another profound truth , where everything is interdependent , multicausal , interconnected : language shapes culture while culture is shaping language ; language shapes thinking while thinking is shaping language . the cumulative effect of the ( humboldt / boas / sapir - ) whorf hypothesis literature has been primarily to throw up a smokescreen around his ideas so that people , including grad students in linguistics , psychology , anthropology and sociology , won\\'t read him in the original ( english ! ) . i tell my grad students that if they want to really find out what their discipline is about , go find out who their discipline is beating up on and read them ; and if you are so lucky as me to find someone that four major academic disciplines are ganging up on - - you know you \\' ve hit a goldmine ! what in the world could be so important that four academic disciplines create a combined smokescreen ? because so few linguists in this century have availed themselves of the changes in thinking about reality that physics has been broadcasting during this entire century , few linguists are even qualified to step into what they did n\\'t realize was an interdisciplinary debate in the history of ideas which whorf felt so comfortable in . i \\' ll explain . i \\' ll give you a synopsis of a talk i intend to give at a 100th birthday conference for benjamin whorf , which i intend to get funding for and hold in the bay area in spring 1997 . i call it \" heisenberg \\'s lament . \" you see , early in this century , that \\' uncertain \\' heisenberg was among the first to gain a \\' glimpse \\' into the subatomic world ; and , having done so , he rendered his opinion that , regarding the subatomic realm , \" we have reached the limits of our language . \" he said this for two reasons : 1 ) no matter how glibly western scientists talk about electrons , protons , neutrons , quarks , etc . , when we look into that realm there are no \\' things \\' , only processes and relationships ; but in order to make sense ( i . e . , complete sentences ) in sae languages , we need nouns - - and there \\'s nothing in the subatomic realm that you can , except willy-nilly , attach nouns to . and 2 ) given that , our most fundamental scientific terms such as \" same \" and \" different \" are useless . he did n\\'t know then , and physics does n\\'t know now , whether there are gazillions of electrons or just one electron with gazillions of manifestations . we have reached the limits of our language . fast - forward a few decades and whorf hears this in his physics classes at yale ( he has unpublished manuscripts on gravity in the yale archives ) , and ponders : hm , i wonder if this has anything to do with what prof . sapir said the other day about hopi not needing nouns to express ordinary propositions - - just \" rehpi \" , \" flashed \" , instead of \" it \" or \" the / a light flashed \" : because when you come right down to it , how is the flashing different from the light ? is \" light \" just a convenient grammatical fiction foisted upon us by sae grammar ? and as he pondered \\' light \\' being noun or verb , particle or wave , depending on how it \\'s viewed , he saw the universe in the same way , with different cultures taking different positions on the question . in this case , since hopi did n\\'t seem to take too seriously the absence of nouns , perhaps , whorf surmised , hopi could be of use for physicists in exploring and reporting back about the quantum world , that realm that did n\\'t have thingy nouns . fast - forward another few decades and physicist david bohm reads whorf ( which i confirmed personally in talking to him ) , and then , * in response * ( my attribution ) , writes _ wholeness and the implicate order _ , in which he , among other things , tries to make english more verby and performative in the \" rheomode \" - - a brilliant flop ; and then launches on the scientific community a view of the universe which does not contain our familiar notions of past / present / future time , but instead an implicate and explicate order of reality - - an \" inny / outty \" notion where the future is inside us working outward instead of some vague distant goal we are headed toward . an email acquaintance pointedly asked me what the difference was between bohm \\'s terminology and whorf \\'s terminology ; it took me 6 months to finally answer that there was none , except the hopis had had theirs for millennia longer . and then it hit me ! bohm , in his own maverick way , appropriated whorf \\'s answer to heisenberg \\'s lament in \" an american indian model of the universe \" and substituted more scientifically acceptable terminology ( implicate / explicate rather than manifesting / manifested ) to see how the notion of a universe without linear time would fly in the modern physics and academic community - - and it had qualified success . but - - bohm was no closer to knowing whether whorf had been accurate in his description of hopi than he had been before writing the book . and there had been so much bad press on the guy ! how was one to know , ultimately ? and here \\'s the part that almost no-one knows so far . in 1991 , in the last few months of his life , david bohm launched his most ambitious thought experiment to date : with some other physicists and a few psychologists and linguists , and sponsored by the fetzer institute , he enticed recognized american indian intellectual leaders ( and some of their elders ) to join in dialogue together in what i can only describe as a roundabout way of asking american indians whether whorf was accurate in his description of the \\' timeless \\' hopi worldview . but it became so much more ! the american indian leaders there had previously read bohm \\'s book and others , but the physicists knew nothing about native american worldviews and native american science methodologies , so the indians had to build a bridge over to them in private meetings before the three public days on the themes of time , space and language . during the day on time , whorf \\'s description of hopi came up , was read out loud , and discussed , though i do n\\'t remember any hopis being present ; nevertheless , the other american indians present , mostly of algonquian tribes , gave what can only be called \\' independent verification \\' in scientific terminology by saying essentially : well , i can\\'t speak for the hopi people , but that \\'s pretty much the way we do it . in fact , of the many whorf passages read or discussed in these dialogues , the physicists and the american indians present were usually willing to give whorf his points ( proving again , perhaps , the difficulty of being a prophet in one \\'s own country / discipline ! ) . but we \\' re not done yet - - the best is yet to come - - the actual conclusion of heisenberg \\'s lament ! at the beginning of the first dialogue it was clear that the quantum physicists had their favorite realm to explore and talk about , and the american indians also had their own favorite realm . as we dialogued , it began becoming clear that those favorite realms had some fundamental principles in common : the only constant is flux ; everything that exists vibrates ; everything is interconnected such that the part implicates the whole . in fact , it became crystal clear that the last major obstacle to these realms being the same realm was really only terminological : the physicists are used to calling it \" the subatomic realm \" whereas the american indians for millennia have been calling it \" the spirit realm \" . now that \\'s a big enough surprise - - that modern physics is knocking on the door of spirit without really meaning to - - but not big enough , so now let \\'s take it home ! it puzzled the physicists just how the american indians should have foreknowledge of a realm they should n\\'t know about , that the western scientific infrastructure had just recently led us to - - and the indians had no such scientific infrastructure ! and as the physicists gradually understood that , like hopi , the algonquian grammatical structures do not demand nouns , do not demand fictitious actors to embody actions ( that , as my mikmaq and blackfoot friends tell me , they can talk all day long in those languages and never utter a single \\' noun \\' ! ) , they finally had to admit that such languages were indeed much better suited to exploring that realm and reporting back than sae languages - - whorf \\'s reply to heisenberg \\'s lament was verified and agreed upon . when the phenomena of reality change in a dramatic way , you need to change the tool you \\' re using . now , of course , the physicists were left with an even larger puzzle , to wit : how is it that these american indians have a language much better suited than sae languages to investigate and describe the inner workings of the subatomic realm - - a realm they are n\\'t even supposed to know about ! ? ! as you can see by now , pinker - - like all other facile critics and unindicted co-creators of the so-called hypothesis - - is out of his league altogether in attempting to characterize a major player in one of the most important interdisciplinary discussions ever in the history of ideas . pinker , like chomsky , loves logic ( which grows out of the grammar of sae languages just as the philosophy of \\' karma \\' grew out of the grammar of ancient sanskrit , where it was used earliest as the linguistics term for \\'d irect object \\' ! ! ) , but has never really gotten with the program this century to replace binary / dualistic thinking with multivariable / multicausal / interdependent systems thinking . whorf heard the call , way back then , and may yet prove to have been an entire century ahead of his time in linguistics . even though we can think in systems for phonology and grammar , we have a tough time doing it for \" language and thought \" ; we feel we have to make them bipolar opposites such that they are distinct and one causes the other invariantly ; that \\'s why i so admired how slobin finally lost those monolithic terms and framed the question instead in terms of \" thinking for speaking \" - - the at least one kind of thinking where your thinking is very much at the mercy of the forms and categories of your language , per whorf . so tell your students , as i do , that the only way to get to the bottom of what whorf did or did not say is to read his essays in _ language , thought & reality _ for themselves , perhaps with the above thoughts as a guideline , and then figure out for themselves whether the sapir - whorf hypothesis smokescreen makes any sense .',\n",
       "       'Subject: call for contributions  call for contributions : _ concepts and practice of network - based language teaching _ mark warschauer , university of hawaii at manoa richard kern , university of california at berkeley we are submitting a proposal to cambridge university press applied linguistics series ( series editors michael long and jack richards ) for an edited volume on the concepts and practice of computer network-based language teaching ( i . e . , involving the internet , local area networks , or other forms of electronic communication ) . it is intended that the book will be solidly based on second language acquisition theory and research and that its principle audience will be faculty and graduate students ( e . g . , as a text in graduate courses in applied linguistics , tesol , and foreign language education ) . we are seeking two types of chapter submissions : ( 1 ) critical analyses of the concepts of network-based teaching as they relate to aspects of language acquistion theory or educational theory ( for example , consideration of relationships of network-based language teaching to cognitive , psycholinguistic , sociolinguistic , sociocultural , literary , or critical pedagogical theories ) . ( 2 ) theoretically - grounded empirical studies of the practice of network-based teaching . chapters on classroom practice should include a review of the literature , a detailed description of the research methods used , an in-depth analysis and discussion of the data , and implications for teaching and future research . analyses can be qualititative or quantitative , and can explore multiple types of variables ( e . g . , process , product , cognitive , social , affective , contextual ) . timeline / deadlines : 1 . dec . 1 , 1996 : notification of interest please send an email message to mark warschauer ( mark @ hawaii . edu ) or richard kern ( kernrg @ uclink . berkeley . edu ) notifying us of your possible interest in submitting an abstract as well as the likely topic . 2 . jan . 15 , 1996 : submission of abstract please send one packet to each editor including : one page with the title of your abstract and your and your contact information ( address , telephone , e-mail , and fax number ) ; one page with the title and abstract of the proposed chapter ( maximum 1 - 2 pages , single spaced ) ; your complete cv , including previous publications one copy to : one copy to : mark warschauer richard kern esl dept , moore 570 department of french 1890 east - west road university of california , berkeley university of hawaii berkeley , ca 94720-2580 honolulu , hi 96816 3 . feb . 15 , 1996 : notification about status of abstract 4 . sept . 1 , 1996 : manuscripts submitted to editors ( warschauer / kern ) ( hard copy and diskette , in apa format ) 5 . oct . 1 , 1996 : initial editorial response ( by warschauer / kern ) to manuscripts 6 . dec . 15 , 1996 : revised manuscripts due 7 . feb 1 , 1997 : book manuscript submitted to cambridge university press applied linguistics series editors ( expected publication date , 9-12 months later ) the editors : mark warschauer is a researcher at the national foreign language resource center of the university of hawaii . his publications include _ e - mail for english teaching : bringing the internet and computer learning networks into the language classroom _ ( tesol publications , 1995 ) and _ virtual connections : online activities & projects for networking language learners _ ( university of hawaii , in press ) . richard kern is assistant professor of french and director of the french language program at the university of california at berkeley . his research interests include reading and writing in a foreign language and the use of networked computers to facilitate communicative language use . he has published articles in the modern language journal , foreign language annals , canadian modern language review , and studies in second language acquisition . thank you very much for your interest . we hope that this book will play an important role in bringing together the most advanced research on this topic and making it available to faculty , researchers , graduate students , and interested teachers . we are looking forward to hearing from you and to receiving your abstracts . mark warschauer richard kern university of hawaii university of california , berkeley markw @ hawaii . edu kernrg @ uclink . berkeley . edu'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### a. emails correstponding to\n",
    "emails.message.values[982:985]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26c2ee2a-f2da-477a-8252-b18fd64da4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nooteboom', 'nootka', 'nope', 'nor', 'nora']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b. words corresponding to\n",
    "vocabulary[40041:40046]\n",
    "\n",
    "# c. 1 indicates that the corresponding email contains the word\n",
    "\n",
    "# d. 0 indicates that the corresponding email does not contain the word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb70dba8-2690-4fb6-9c05-b57b36b5ebfb",
   "metadata": {},
   "source": [
    "### 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0f1c2b9-6d75-4d26-b664-b3c7e561424d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8337366055997235\n"
     ]
    }
   ],
   "source": [
    "spam_count = np.count_nonzero(Y)\n",
    "non_spam_count = np.size(Y) - np.count_nonzero(Y)\n",
    "\n",
    "# majority classfier accuracy\n",
    "if spam_count > non_spam_count:\n",
    "  print(np.mean(Y))\n",
    "else:\n",
    "  print(1 - np.mean(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8e14aa-6c32-4e90-92d4-b8f31e8025ea",
   "metadata": {},
   "source": [
    "### 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1c56679-e541-4c5f-a8cc-fbfc4d142c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-bd18f733a0bb>:3: RuntimeWarning: divide by zero encountered in log\n",
      "  lg_P_W1_S1 = np.log(Xt.toarray()[Yt].mean(axis = 0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Pr(W=1|S=1) [-1.13526561 -1.11939226 -5.96357934 ...        -inf -5.96357934\n",
      "        -inf]\n",
      "Log Pr(W=1|S=0) [-1.82288833 -2.89924215 -7.56268125 ... -7.56268125        -inf\n",
      "        -inf]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-bd18f733a0bb>:4: RuntimeWarning: divide by zero encountered in log\n",
      "  lg_P_W1_S0 = np.log(Xt.toarray()[~Yt].mean(axis = 0))\n"
     ]
    }
   ],
   "source": [
    "lg_P_S1 = np.log(np.mean(Yt))\n",
    "lg_P_S0 = np.log(np.mean(~Yt))\n",
    "lg_P_W1_S1 = np.log(Xt.toarray()[Yt].mean(axis = 0)) \n",
    "lg_P_W1_S0 = np.log(Xt.toarray()[~Yt].mean(axis = 0)) \n",
    "print('Log Pr(W=1|S=1)', lg_P_W1_S1)\n",
    "print('Log Pr(W=1|S=0)', lg_P_W1_S0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9991ca33-97b2-4124-a088-30f6da701d24",
   "metadata": {},
   "source": [
    "### 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4f1b36b-0bae-4929-923b-3b8893d4c867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[       -inf,        -inf,        -inf, ..., -7.74673291,\n",
       "        -6.64812062, -0.18405166],\n",
       "       [       -inf,        -inf,        -inf, ...,        -inf,\n",
       "               -inf, -1.78315356]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_prob(content):\n",
    "    idx = np.where(content == 1)\n",
    "    tmpt_lg_P_S1 = lg_P_S1 + np.sum(lg_P_W1_S1[np.array(idx)])\n",
    "    tmpt_lg_P_S0 = lg_P_S0 + np.sum(lg_P_W1_S0[np.array(idx)])\n",
    "    return [tmpt_lg_P_S0, tmpt_lg_P_S1]\n",
    "result0 = np.apply_along_axis(compute_prob, 0, Xt.toarray())\n",
    "result0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cc561be-b99d-45a2-88d3-efa5c1d9ac2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -356.76747718,           -inf],\n",
       "       [-1430.57828432,           -inf],\n",
       "       [ -265.00881529,           -inf],\n",
       "       ...,\n",
       "       [ -344.03740529,           -inf],\n",
       "       [ -276.22824202,           -inf],\n",
       "       [-2478.11318621,           -inf]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1 = np.apply_along_axis(compute_prob, 1, Xt.toarray())\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5636c59f-9d58-411a-87f7-eac2e1a25a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 60925)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec1d2798-87b5-4250-9508-e5919abf616b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2314, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f866c-882f-408b-a6b9-4f8063ffd93e",
   "metadata": {},
   "source": [
    "### 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d758156-24d7-4697-a0d0-59022a0a7f9b",
   "metadata": {},
   "source": [
    "We have to compute 2 dimension (probabilities) for each email, because we have to compare these probabilities to classify the email as spam or non-spam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd9af41-4e6b-4cb8-810e-032cc09848ea",
   "metadata": {},
   "source": [
    "### 2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5de7219f-0656-4605-a1e1-f4841631a29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(content):\n",
    "    idx = np.where(content == 1)\n",
    "    tmpt_lg_P_S1 = lg_P_S1 + np.sum(lg_P_W1_S1[np.array(idx)])\n",
    "    tmpt_lg_P_S0 = lg_P_S0 + np.sum(lg_P_W1_S0[np.array(idx)])\n",
    "    if tmpt_lg_P_S1 > tmpt_lg_P_S0:\n",
    "        return True\n",
    "    else:\n",
    "        return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cec24f6-a707-48f7-99b1-6c855e1bb94b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(579,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict = np.apply_along_axis(predict, 1, Xv.toarray())\n",
    "Y_predict.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b24d8e5-f899-4f13-8ae8-b0ba4f9e890e",
   "metadata": {},
   "source": [
    "### 2.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dd3b2a-6780-4a26-b701-af45a8c2e6a2",
   "metadata": {},
   "source": [
    "We have need higher the value of the log-likelihood(579), which results in a better model fitting a dataset, and it is useful for comparing two or more models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1401c91f-ba4c-4a7b-9074-b92a2d37a82e",
   "metadata": {},
   "source": [
    "### 2.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b44a85d7-c63f-4d7f-83be-39aff457dc8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[       -inf,        -inf,        -inf, ..., -7.74673291,\n",
       "        -6.64812062, -0.18405166],\n",
       "       [       -inf,        -inf,        -inf, ...,        -inf,\n",
       "               -inf, -1.78315356]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(content):\n",
    "    idx = np.where(content == 1)\n",
    "    tmpt_lg_P_S1 = lg_P_S1 + np.sum(lg_P_W1_S1[np.array(idx)])\n",
    "    tmpt_lg_P_S0 = lg_P_S0 + np.sum(lg_P_W1_S0[np.array(idx)])\n",
    "    return [tmpt_lg_P_S0, tmpt_lg_P_S1]\n",
    "S0 = np.apply_along_axis(predict, 0, Xt.toarray())\n",
    "S0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20610106-35f8-464f-8548-523476e970ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -356.76747718,           -inf],\n",
       "       [-1430.57828432,           -inf],\n",
       "       [ -265.00881529,           -inf],\n",
       "       ...,\n",
       "       [ -344.03740529,           -inf],\n",
       "       [ -276.22824202,           -inf],\n",
       "       [-2478.11318621,           -inf]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S1 = np.apply_along_axis(predict, 1, Xt.toarray())\n",
    "S1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6179b229-855c-4388-9d19-eba19f5be493",
   "metadata": {},
   "source": [
    "### 2.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ff33fa9-96a2-495a-ab3e-70595921708d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[487,   0],\n",
       "       [ 80,  12]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(Yv, Y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41f87d76-ea5e-4eaf-bb26-92fe5d92f8d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8618307426597582"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(Yv, Y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78135d91-87d1-40dc-b44b-d1544166a67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8411053540587219"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - np.mean(Yv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd58818a-6edf-4307-8a38-22afccc33557",
   "metadata": {},
   "source": [
    "### 2.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5564086-6cc1-4599-a8b7-e31eaf9fa714",
   "metadata": {},
   "source": [
    "The cause of such a problem of many infinities was the 0 probability of count of word with regards to few cases of the word, relying on a single rare value for categorization. Thus, we need smoothing methodadding a\n",
    "small positive number to the counts to caculate probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec608bf-4ac5-492b-93ec-0ffb2e9e79be",
   "metadata": {},
   "source": [
    "### 3.1 & 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "355470f3-0d2d-4e8d-94b3-4c0d3cf119f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitting(Xt, Yt, alpha):\n",
    "    n_spam = np.sum(Yt) + alpha\n",
    "    n_non_spam = np.sum(~Yt) + alpha\n",
    "    n_total = n_spam + n_non_spam\n",
    "    spam_counts = Xt.toarray()[Yt].sum(0) + alpha\n",
    "    non_spam_counts = Xt.toarray()[~Yt].sum(0) + alpha\n",
    "    lg_P_S1 = np.log(n_spam / n_total) \n",
    "    lg_P_S0 = np.log(n_non_spam / n_total) \n",
    "    lg_P_W1_S1 = np.log(spam_counts / (n_spam + alpha)) \n",
    "    lg_P_W1_S0 = np.log(non_spam_counts / (n_non_spam + alpha))\n",
    "\n",
    "    return lg_P_S1, lg_P_S0, lg_P_W1_S1, lg_P_W1_S0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b136df-4430-4398-8c6b-fe80be20d072",
   "metadata": {},
   "source": [
    "### 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0cfa259b-9e43-43d2-9f8e-ae82980b1b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-794c1d16b53c>:9: RuntimeWarning: divide by zero encountered in log\n",
      "  lg_P_W1_S1 = np.log(spam_counts / (n_spam + alpha))\n",
      "<ipython-input-29-794c1d16b53c>:10: RuntimeWarning: divide by zero encountered in log\n",
      "  lg_P_W1_S0 = np.log(non_spam_counts / (n_non_spam + alpha))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-ef814fe20ea4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mY_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Alpha:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Confusion Matrix:\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \"\"\"\n\u001b[0;32m--> 296\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtype_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_pred\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mtype_of_target\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'f'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;31m# [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m'continuous'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m    102\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    104\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     (type_err,\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "for alpha in range(5):\n",
    "    lg_P_S1, lg_P_S0, lg_P_W1_S1, lg_P_W1_S0 = fitting(Xt, Yt, alpha / 10)\n",
    "    Y_predict = np.apply_along_axis(predict, 1, Xv.toarray())\n",
    "    print('Alpha:', alpha / 10)\n",
    "    print('Confusion Matrix:\\n', confusion_matrix(Yv, Y_predict))\n",
    "    print('Accuracy:', accuracy_score(Yv, Y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb79a769-5d53-4161-bb29-9380bb7b0340",
   "metadata": {},
   "source": [
    "### 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c26db20-bae5-4f80-8316-475eb01b7890",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-5d9462119f44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlg_P_S1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlg_P_S0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlg_P_W1_S1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlg_P_W1_S0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfitting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mY_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Alpha:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Confusion Matrix:\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0m\u001b[1;32m     93\u001b[0m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous-multioutput targets"
     ]
    }
   ],
   "source": [
    "alphas = np.logspace(-8, 1, num=10)\n",
    "accuracies = []\n",
    "for i in alphas:\n",
    "    lg_P_S1, lg_P_S0, lg_P_W1_S1, lg_P_W1_S0 = fitting(Xt, Yt, i)\n",
    "    Y_predict = np.apply_along_axis(predict, 1, Xv.toarray())\n",
    "    accuracies.append(accuracy_score(Yv, Y_predict))\n",
    "    print('Alpha:', i)\n",
    "    print('Confusion Matrix:\\n', confusion_matrix(Yv, Y_predict))\n",
    "    print('Accuracy:', accuracy_score(Yv, Y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc337e2-7f5c-43f7-a02f-a099bbd18c24",
   "metadata": {},
   "source": [
    "### 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9b1a47-a370-4052-90c2-a61c1a691145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_ = plt.plot(np.log10(alphas), accuracies, marker=\"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8815de97-346f-4894-945a-7073d47a94ff",
   "metadata": {},
   "source": [
    "### 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a79ce015-c87f-43df-b767-68b17c69857e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-fbba303bb4ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mbest_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlg_diffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mworst_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlg_diffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mbest_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mworst_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mworst_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbest_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworst_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "# probabilities with best accuracy\n",
    "lg_P_S1, lg_P_S0, lg_P_W1_S1, lg_P_W1_S0 = fitting(Xt, Yt, 0.001)\n",
    "lg_P_W1 = np.log((np.sum(Xt) + 0.001) / (Xt.shape[0] + 0.002))\n",
    "lg_P_S1_W1 = lg_P_W1_S1 * lg_P_S1 / lg_P_W1\n",
    "lg_P_S0_W1 = lg_P_W1_S0 * lg_P_S0 / lg_P_W1\n",
    "lg_diffs = lg_P_S1_W1 - lg_P_S0_W1\n",
    "best_idx = np.argsort(-lg_diffs)[:10]\n",
    "worst_idx = np.argsort(lg_diffs)[:10]\n",
    "best_vocab = vocabulary[best_idx]\n",
    "worst_vocab = vocabulary[worst_idx]\n",
    "(best_vocab, worst_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c008138-37ef-492a-89e5-c1da51c905e3",
   "metadata": {},
   "source": [
    "### 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0ba2e1-e1c2-4d0c-a3a8-f38ac5ddc602",
   "metadata": {},
   "source": [
    "comment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac048c05-2a3d-40e5-9609-eb1cbeca9803",
   "metadata": {},
   "source": [
    "### 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e926021-5853-43e4-a003-2620c1e068ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def KFold_CV(X, Y, alpha, k):\n",
    "    kf = KFold(n_splits=k)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "        m = GaussianNB()\n",
    "        _ = m.fit(X_train, Y_train)\n",
    "        return m.score(X_train, Y_train)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eaacaabd-0d0b-4a7e-8312-9ad60169f223",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "KFold_CV() missing 1 required positional argument: 'k'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-680f3b2cf4f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m#lg_P_S1, lg_P_S0, lg_P_W1_S1, lg_P_W1_S0 = fitting(Xt, Yt, alpha / 10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m#Y_predict = np.apply_along_axis(predict, 1, Xv.toarray())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mKFold_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold_CV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKFold_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: KFold_CV() missing 1 required positional argument: 'k'"
     ]
    }
   ],
   "source": [
    "alphas = np.logspace(-8, 1, num=10)\n",
    "accuracies = []\n",
    "for i in alphas:\n",
    "  #lg_P_S1, lg_P_S0, lg_P_W1_S1, lg_P_W1_S0 = fitting(Xt, Yt, alpha / 10)\n",
    "  #Y_predict = np.apply_along_axis(predict, 1, Xv.toarray())\n",
    "    KFold_accuracy = KFold_CV(X, Y, alpha/10)\n",
    "    accuracies.append(accuracy_score(Yv, Y_predict))\n",
    "    print('Accuracy:', KFold_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2a1d6cd-c0a2-4f2a-a2f4-884ab0fc239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def cross_val(params):\n",
    "    model = GaussianNB()\n",
    "    model.set_params(**params)\n",
    "    cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "    cv_results = cross_val_score(model, X_train, y_train,\n",
    "                             cv = 10, #10 folds\n",
    "                             scoring = \"accuracy\",\n",
    "                             verbose = 2\n",
    "                            )\n",
    "    #return the mean of the 10 fold cross validation\n",
    "    return cv_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c63396-ce51-424a-9df6-768e01494277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e86e87-c62f-4d44-9395-dabd45cd00c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_train, y_train, X_valid, y_valid in k_fold_generator(X, y, k_fold):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
